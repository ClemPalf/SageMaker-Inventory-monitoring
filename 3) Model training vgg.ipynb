{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Model training\n",
    "In this notebook, we will train an entire pretrained VGG model (not just the classifications layers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install smdebug\n",
    "!pip install torchvision --no-cache-dir  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-16 09:25:17.151 datascience-1-0-ml-t3-medium-1abf3407f667f989be9d86559395:22 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "Default Bucket: sagemaker-us-east-1-837030799965\n",
      "AWS Region: us-east-1\n",
      "RoleArn: arn:aws:iam::837030799965:role/service-role/AmazonSageMaker-ExecutionRole-20211207T163039\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.debugger import DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import IPython\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "prefix = \"capstone-inventory-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_vgg.py file contains the training script.  \n",
    "While the whole model will be trained (for 10 epochs), the weights are pretrained.  \n",
    "models.vgg11_bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-15 16:51:14 Starting - Starting the training job."
     ]
    }
   ],
   "source": [
    "# Create and fit an estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"scripts/train.py\",\n",
    "    role=role,\n",
    "    py_version='py36',\n",
    "    framework_version=\"1.8\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",        \n",
    "    profiler_config=profiler_config,      # profiler hook\n",
    "    debugger_hook_config=debugger_config, # debugger hook\n",
    "    output_path = 's3://{}/{}/main_training'.format(bucket, prefix),  # The training jobs output (mainly model artefacts) will go there.\n",
    "    hyperparameters={                                                 # Best values from the previous hpo\n",
    "        \"batch-size\": 16, # Hyperparameters obtain from our hpo\n",
    "        \"lr\": 0.002}\n",
    ")\n",
    "\n",
    "estimator.fit({\"train\": \"s3://{}/{}/data\".format(bucket, prefix)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are quite disappointing as the model did not manage to improve at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/results-training.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see two possible reasons:\n",
    "- The model type is not appropriate to capture the wide range of features which can define the number of objects in a bin.\n",
    "- The model should have been trained from scratch. To use pretrained weights might have stuck the model into a local minima right at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook, we will try to mitigate this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
