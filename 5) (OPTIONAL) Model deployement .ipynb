{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Model deployement\n",
    "In this notebook, we will deploy the model, set up autoscalling, and finally invoke the endpoint a few times as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Bucket: sagemaker-us-east-1-646714458109\n",
      "AWS Region: us-east-1\n",
      "RoleArn: arn:aws:iam::646714458109:role/service-role/AmazonSageMaker-ExecutionRole-20211122T183493\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "print(\"Default Bucket: {}\".format(bucket))\n",
    "\n",
    "region = session.boto_region_name\n",
    "print(\"AWS Region: {}\".format(region))\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "prefix = \"capstone-inventory-project\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first deploy our model, we provided our own inference script \"inference.py\".  \n",
    "To keep track of the model inferences, we will attach a datacapture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"s3://{bucket}/{prefix}/data_capture\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = PyTorchModel(model_data=\"s3://sagemaker-us-east-1-646714458109/capstone-inventory-project/main_training/pytorch-training-2022-01-17-10-08-10-837/output/model.tar.gz\", \n",
    "                             role=role, \n",
    "                             entry_point='scripts/inference.py',\n",
    "                             py_version='py3',\n",
    "                             framework_version='1.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Elastic Inference will be used to optimize inference speed cost-effectively. A low-cost GPU-powered acceleration will be attached to the deployed EC2 instance. This configuration tends to reduce costs up to 75% compared to traditional  GPU instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, \n",
    "                                 data_capture_config=data_capture_config,\n",
    "                                 instance_type='ml.m5.large',\n",
    "                                 accelerator_type='ml.eia2.medium' # Low cost GPU\n",
    "                                )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoscalling\n",
    "To reduce potential latency, a custom sclaing policy will be setup.  \n",
    "Up to 3 instances can be instantiated to meet demand based on CPU usage. More specificaly, if an endpoint has an average CPU utilization of more than 70% for more than 30sc, another endpoint will be deployed. This policy was implemented following this [documentaion](https://aws.amazon.com/blogs/machine-learning/configuring-autoscaling-inference-endpoints-in-amazon-sagemaker/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'CreationTime': datetime.datetime(2022, 1, 18, 12, 59, 2, 417000, tzinfo=tzlocal()),\n",
      "    'DataCaptureConfig': {   'CaptureStatus': 'Started',\n",
      "                             'CurrentSamplingPercentage': 100,\n",
      "                             'DestinationS3Uri': 's3://sagemaker-us-east-1-646714458109/capstone-inventory-project/data_capture',\n",
      "                             'EnableCapture': True},\n",
      "    'EndpointArn': 'arn:aws:sagemaker:us-east-1:646714458109:endpoint/pytorch-inference-eia-2022-01-18-12-59-02-036',\n",
      "    'EndpointConfigName': 'pytorch-inference-eia-2022-01-18-12-59-02-036',\n",
      "    'EndpointName': 'pytorch-inference-eia-2022-01-18-12-59-02-036',\n",
      "    'EndpointStatus': 'InService',\n",
      "    'LastModifiedTime': datetime.datetime(2022, 1, 18, 13, 7, 24, 717000, tzinfo=tzlocal()),\n",
      "    'ProductionVariants': [   {   'CurrentInstanceCount': 1,\n",
      "                                  'CurrentWeight': 1.0,\n",
      "                                  'DeployedImages': [{...}],\n",
      "                                  'DesiredInstanceCount': 1,\n",
      "                                  'DesiredWeight': 1.0,\n",
      "                                  'VariantName': 'AllTraffic'}],\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'content-length': '994',\n",
      "                                               'content-type': 'application/x-amz-json-1.1',\n",
      "                                               'date': 'Tue, 18 Jan 2022 '\n",
      "                                                       '13:08:04 GMT',\n",
      "                                               'x-amzn-requestid': '1e76eb1e-3ed6-45b7-9089-903c1947537f'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'RequestId': '1e76eb1e-3ed6-45b7-9089-903c1947537f',\n",
      "                            'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "role = get_execution_role()\n",
    "sagemaker_client = boto3.Session().client(service_name='sagemaker')\n",
    "endpoint_name = 'pytorch-inference-eia-2022-01-18-12-59-02-036'\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "pp.pprint(response)\n",
    "\n",
    "#Let us define a client to play with autoscaling options\n",
    "client_auto = boto3.client('application-autoscaling') # Common class representing Application Auto Scaling for SageMaker amongst other services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_id='endpoint/' + endpoint_name + '/variant/' + 'AllTraffic' # This is the format in which application autoscaling references the endpoint \"AllTraffic\" is the auto assign variant to the endpoint\n",
    "\n",
    "response = client_auto.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client_auto.put_scaling_policy(\n",
    "    PolicyName='CPUUtil-ScalingPolicy',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 70.0,\n",
    "        'CustomizedMetricSpecification':\n",
    "        {\n",
    "            'MetricName': 'CPUUtilization',\n",
    "            'Namespace': '/aws/sagemaker/Endpoints',\n",
    "            'Dimensions': [\n",
    "                {'Name': 'EndpointName', 'Value': endpoint_name },\n",
    "                {'Name': 'VariantName','Value': 'AllTraffic'}\n",
    "            ],\n",
    "            'Statistic': 'Average', # Possible - 'Statistic': 'Average'|'Minimum'|'Maximum'|'SampleCount'|'Sum'\n",
    "            'Unit': 'Percent'\n",
    "        },\n",
    "        'ScaleInCooldown': 30,\n",
    "        'ScaleOutCooldown': 30\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy was correctly implemented:  \n",
    "![alt text](images/scaling_policy.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke using boto3 client\n",
    "Let's first try to invoke the endpoint directly using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"images/test.jpg\", \"rb\") as f:\n",
    "    image_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName = endpoint_name,      # The name of the endpoint we created\n",
    "                                   ContentType = 'image/jpeg',         # The data format that is expected\n",
    "                                   Body = image_data)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.5246334075927734,\n",
       "  -0.3356003761291504,\n",
       "  0.7459352612495422,\n",
       "  1.120079517364502,\n",
       "  1.032776117324829]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke using lambda (by providing url)\n",
    "Let's now make a prediction using a lambda function as an intermediate (the lambda function code is available in the lambda.py file). \n",
    "1) We invoke the lambda function, and send an url (pointing the S3 image) as payload.   \n",
    "2) The function invoke the endpoint using the url as input.  \n",
    "3) The endpoint download the image, performance inference, and return the prediction to the lambda function.  \n",
    "4) The prediction is returned by the lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke(\n",
    "    FunctionName='inference_capstone',\n",
    "    Payload='{\"url\": \"https://sagemaker-us-east-1-646714458109.s3.amazonaws.com/capstone-inventory-project/data/train/1/00014.jpg\"}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '37027ec4-9054-4840-9b09-909305be2102', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 18 Jan 2022 13:12:33 GMT', 'content-type': 'application/json', 'content-length': '131', 'connection': 'keep-alive', 'x-amzn-requestid': '37027ec4-9054-4840-9b09-909305be2102', 'x-amzn-remapped-content-length': '0', 'x-amz-executed-version': '$LATEST', 'x-amzn-trace-id': 'root=1-61e6bcbf-0f4793ac7357c1fa22c31f4e;sampled=0'}, 'RetryAttempts': 0}, 'StatusCode': 200, 'ExecutedVersion': '$LATEST', 'Payload': <botocore.response.StreamingBody object at 0x7f499468a910>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2.5246334075927734,\n",
       "  -0.3356003761291504,\n",
       "  0.7459352612495422,\n",
       "  1.120079517364502,\n",
       "  1.032776117324829]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response['Payload'].read().decode())[\"body\"]"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
